{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages**"
      ],
      "metadata": {
        "id": "mvZenTPQv0t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install nltk package\n",
        "!pip install pandas nltk\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "M8BlGFyav1hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove annoying warnings from sklearn\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "metadata": {
        "id": "4S2JQb4MrDg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import all packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "#import wikipedia as wiki\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "nltk.download('punkt') # downloads the NLTK data for tokenization\n",
        "nltk.download('stopwords') # downloads the NLTK data for stopwords\n",
        "nltk.download('wordnet') # downloads the NLTK data for WordNet lemmatizer"
      ],
      "metadata": {
        "id": "0crLbIm2MoBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "GpNRZOHUv4lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # removes punctuation from the text using the translate method\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text) # tokenizes the text into individual words\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english')) # creates a set of stopwords for English language\n",
        "    tokens = [w for w in tokens if not w in stop_words] # removes stopwords from the list of tokens\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer() # creates an instance of the WordNetLemmatizer\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # applies lemmatization to each token in the list\n",
        "    return ' '.join(tokens)\n",
        "    #return tokens\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_json('/content/News_Category_Dataset_v3.json', lines=True)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "# Preprocess the news headline & short description\n",
        "df['processed_text'] = df.apply(lambda row: preprocess_text(row['headline'] + \" \" + row['short_description']), axis=1)\n",
        "print(df['processed_text'])"
      ],
      "metadata": {
        "id": "W9jk5aiHxhIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the frequency of each category\n",
        "category_counts = df['category'].value_counts()\n",
        "\n",
        "# Plot the category distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(category_counts.index, category_counts.values)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Categories')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HrwqJE5yocbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_categories = category_counts.nunique()\n",
        "print(\"Total number of categories:\", total_categories)\n",
        "category_counts = df['category'].value_counts()\n",
        "print(category_counts)"
      ],
      "metadata": {
        "id": "_fuhJW2ioceM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization**"
      ],
      "metadata": {
        "id": "gxppIRUHYDAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "processed_text = df['processed_text'].tolist()\n",
        "\n",
        "# Word2Vec Vectorization\n",
        "tokenized_articles = [text.split() for text in processed_text]\n",
        "word2vec_model = Word2Vec(tokenized_articles, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "document_vector = []\n",
        "for article in tokenized_articles:\n",
        "    for word in article:\n",
        "        if word in word2vec_model.wv.key_to_index:\n",
        "            document_vector.append(word2vec_model.wv[word])\n",
        "\n",
        "# Print the word vectors in batches\n",
        "batch_size = 32\n",
        "for i in range(0, len(document_vector), batch_size):\n",
        "    batch_vectors = document_vector[i:i+batch_size]\n",
        "    print(batch_vectors)\n",
        "\n",
        "# Alternatively, you can print a subset of word vectors\n",
        "subset_size = 100\n",
        "print(document_vector[:subset_size])"
      ],
      "metadata": {
        "id": "iWM1fNeIGJY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction**"
      ],
      "metadata": {
        "id": "WBkFJbJOYAs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensionality Reduction\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "document_array = np.array(document_vector)\n",
        "pca = PCA(n_components=2)\n",
        "document_pca = pca.fit_transform(document_array)\n",
        "print(document_pca)"
      ],
      "metadata": {
        "id": "N6LVRegBWhsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering Algorithm**"
      ],
      "metadata": {
        "id": "CJbzZjWKX-kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Generate sample data\n",
        "X, _ = make_blobs(n_samples=100, centers=23, random_state=42)\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Check the number of distinct samples in the dataset\n",
        "unique_samples = np.unique(X_scaled, axis=0)\n",
        "n_samples = unique_samples.shape[0]\n",
        "\n",
        "# Check if the number of clusters is valid\n",
        "n_clusters = 42\n",
        "if n_clusters > n_samples:\n",
        "    n_clusters = n_samples\n",
        "\n",
        "# Disable ConvergenceWarnings for demonstration purposes\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(unique_samples)\n",
        "\n",
        "# Agglomerative clustering\n",
        "agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "agglomerative_labels = agglomerative.fit_predict(unique_samples)\n",
        "\n",
        "# GMM clustering\n",
        "gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "gmm.fit(unique_samples)\n",
        "gmm_labels = gmm.predict(unique_samples)\n",
        "\n",
        "# Enable ConvergenceWarnings\n",
        "warnings.filterwarnings(\"default\", category=UserWarning)\n",
        "\n",
        "# Print the labels assigned by each clustering algorithm\n",
        "print(\"K-means labels:\", kmeans_labels)\n",
        "print(\"Agglomerative labels:\", agglomerative_labels)\n",
        "print(\"GMM labels:\", gmm_labels)"
      ],
      "metadata": {
        "id": "hp-3sEkBl1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of Clustering & Comparison**"
      ],
      "metadata": {
        "id": "i0lmdPawm_CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate Calinski-Harabasz scores\n",
        "kmeans_ch_score = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
        "agglomerative_ch_score = calinski_harabasz_score(X_scaled, agglomerative_labels)\n",
        "gmm_ch_score = calinski_harabasz_score(X_scaled, gmm_labels)\n",
        "\n",
        "# Print the Calinski-Harabasz scores\n",
        "print(\"K-means Calinski-Harabasz Index:\", kmeans_ch_score)\n",
        "print(\"Agglomerative Calinski-Harabasz Index:\", agglomerative_ch_score)\n",
        "print(\"GMM Calinski-Harabasz Index:\", gmm_ch_score)\n",
        "\n",
        "# Plot the clustering results\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# K-means clustering\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels)\n",
        "plt.title(\"K-means Clustering\")\n",
        "\n",
        "# Agglomerative clustering\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agglomerative_labels)\n",
        "plt.title(\"Agglomerative Clustering\")\n",
        "\n",
        "# GMM clustering\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=gmm_labels)\n",
        "plt.title(\"GMM Clustering\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine the best clustering algorithm based on the Calinski-Harabasz index\n",
        "best_ch_score = max(kmeans_ch_score, agglomerative_ch_score, gmm_ch_score)\n",
        "best_algorithm = None\n",
        "\n",
        "if best_ch_score == kmeans_ch_score:\n",
        "    best_algorithm = \"K-means\"\n",
        "elif best_ch_score == agglomerative_ch_score:\n",
        "    best_algorithm = \"Agglomerative\"\n",
        "else:\n",
        "    best_algorithm = \"GMM\"\n",
        "\n",
        "print(\"Best Clustering Algorithm based on Calinski-Harabasz Index:\", best_algorithm)"
      ],
      "metadata": {
        "id": "AkNFANsfmrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCpo1i72QmaF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}