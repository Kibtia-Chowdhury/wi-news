{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages**"
      ],
      "metadata": {
        "id": "mvZenTPQv0t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install nltk package\n",
        "!pip install pandas nltk"
      ],
      "metadata": {
        "id": "M8BlGFyav1hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "GpNRZOHUv4lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all packages\n",
        "import pandas as pd\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt') # downloads the NLTK data for tokenization\n",
        "nltk.download('stopwords') # downloads the NLTK data for stopwords\n",
        "nltk.download('wordnet') # downloads the NLTK data for WordNet lemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # removes punctuation from the text using the translate method\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text) # tokenizes the text into individual words\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english')) # creates a set of stopwords for English language\n",
        "    tokens = [w for w in tokens if not w in stop_words] # removes stopwords from the list of tokens\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer() # creates an instance of the WordNetLemmatizer\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # applies lemmatization to each token in the list\n",
        "    return tokens\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_json('/content/News_Category_Dataset_v3.json', lines=True)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "# Preprocess the news headline & short description\n",
        "df['processed_text'] = df.apply(lambda row: preprocess_text(row['headline'] + \" \" + row['short_description']), axis=1)\n",
        "print(df['processed_text'])"
      ],
      "metadata": {
        "id": "W9jk5aiHxhIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Word2Vec Model**"
      ],
      "metadata": {
        "id": "4dyQiQtVwZ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # imports the Word2Vec class from the gensim.models module\n",
        "sentences = df['processed_text'].tolist() # Prepare sentences: a list of lists of words\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4) # creates Word2Vec model & train the model\n",
        "w2v_model.save(\"news_category_w2v.model\") # saves the trained Word2Vec model"
      ],
      "metadata": {
        "id": "nHm78M7yv1ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Word2Vec Model and Vectorize New Texts**"
      ],
      "metadata": {
        "id": "kK2F6UtBwdGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec # imports the Word2Vec class from the gensim.models module\n",
        "import numpy as np # imports the numpy library\n",
        "\n",
        "w2v_model = Word2Vec.load(\"news_category_w2v.model\") # Load the trained model\n",
        "def vectorize_text(text, model): # vectorize_text takes two parameters\n",
        "    # Preprocess the text to split into words and apply any other preprocessing\n",
        "    words = preprocess_text(text)  # preprocess_text function\n",
        "    word_vectors = np.array([model.wv[word] for word in words if word in model.wv.key_to_index]) # initializes empty array, iterates each word & added to the word_vectors array\n",
        "    if len(word_vectors) > 0: # Check at least one word vector\n",
        "        return np.mean(word_vectors, axis=0) # compute the mean\n",
        "    else:\n",
        "        return np.zeros(model.vector_size) # otherwise return a zero vector\n",
        "dataset['vector'] = dataset['processed_text'].apply(lambda x: vectorize_text(' '.join(x), w2v_model)) # function called the joined preprocessed words & trained Word2Vec model (w2v_model)"
      ],
      "metadata": {
        "id": "r-Je0kSWv1ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_vector(doc, w2v_model):\n",
        "    doc = [word for word in doc if word in w2v_model.wv.key_to_index] # updated to check for word\n",
        "    if not doc: # checks if the doc list is empty\n",
        "        return np.zeros(w2v_model.vector_size) # returns a zero vector\n",
        "    return np.mean([w2v_model.wv[word] for word in doc], axis=0) # computes the mean vector\n",
        "X = np.array([document_vector(doc, w2v_model) for doc in df['processed_text']]) # creates an array X, iterate each document in the processed_text\n",
        "print(X) # prints the array X"
      ],
      "metadata": {
        "id": "bXZVotjGwkh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cluster the Articles**"
      ],
      "metadata": {
        "id": "dEGNfAupwhTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans # import KMeans\n",
        "kmeans = KMeans(n_clusters=5, n_init=10, random_state=42) # it takes several parameters\n",
        "dataset['cluster'] = kmeans.fit_predict(np.vstack(dataset['vector'].values)) # fit the model, vertically stacks the vectors\n",
        "print(dataset['cluster']) # Print cluster labels"
      ],
      "metadata": {
        "id": "4DGrBG8Cv1p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automated Evaluation of Clustering**"
      ],
      "metadata": {
        "id": "dpcK2s50wqiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "kmeans = KMeans(n_clusters=5, n_init=10, random_state=42).fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Silhouette Score\n",
        "silhouette_avg = silhouette_score(X, labels) # computes the average silhouette score\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Davies-Bouldin Index\n",
        "davies_bouldin = davies_bouldin_score(X, labels) # computes the Davies-Bouldin index\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin}\")\n",
        "\n",
        "# Calinski-Harabasz Index\n",
        "calinski_harabasz = calinski_harabasz_score(X, labels) # computes the Calinski-Harabasz index\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")"
      ],
      "metadata": {
        "id": "lkU6gYi7v1sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamically Adjusting Hyperparameters**"
      ],
      "metadata": {
        "id": "HoYg7nNwwt-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = -1 # initializes the variable\n",
        "best_k = 2 # initializes the variable best_k\n",
        "for k in range(2, 10): # starts a loop 2 to 9\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
        "    score = silhouette_score(X, kmeans.labels_) # computes the silhouette score with current value of k\n",
        "    if score > best_score: # check best score\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "print(f\"Best K: {best_k} with Silhouette Score: {best_score}\") # print best k"
      ],
      "metadata": {
        "id": "O-BLR3cawr0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4gqAZLNCKxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqOw717aCK1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}